## view()

改变一个张量的形状。特点是必须是连续的，只有张量是在内存中是连续存储时，才能使用 `view()`。如果一个张量之前经过了 `transpose()`（转置）、`permute()`（置换维度）等操作，它在内存中可能就变得不连续了，这时直接调用 `view()` 会报错。遇到这种情况，先用 `.contiguous()` 方法将张量在内存中重新变为连续的，然后再调用 `view()`。例如：`x = x.transpose(0, 1).contiguous().view(...)`。

使用-1能自动推导维度

```py
# 假设一个批次有10张3x32x32的彩色图片
batch = torch.randn(10, 3, 32, 32)

# 现在想把它变成一个二维矩阵，每一行代表一张图片的所有像素
# 每张图片有 3 * 32 * 32 = 3072 个像素
batch_matrix = batch.view(10, -1) # 或者 batch.view(batch.size(0), -1)

print("改变后形状:", batch_matrix.shape) # 输出: torch.Size([10, 3072])
```

## reshape()

`reshape()`：更“智能”。如果张量是连续的，`reshape()` 的效果和 `view()` 完全一样。如果张量不连续，`reshape()` 会**自动地、隐式地**创建一个数据副本，然后在新副本上进行形状变换，所以它不会报错。

## permute()

**按照你指定的顺序，重新排列张量的维度**。它返回的是原始数据的一个新“视图”（view），这意味着它和 `view()` 一样，操作是高效的，不会复制底层数据，并且返回的张量与原始张量共享存储。

```py
# 一个批次，包含10个句子，每个句子20个词，每个词用50维向量表示
batch_data = torch.randn(10, 20, 50) # 形状: [batch, seq_len, features]
print("原始形状:", batch_data.shape) # 输出: torch.Size([10, 20, 50])

# 模型要求输入形状为 [seq_len, batch, features]
# 原始维度: 0(batch), 1(seq_len), 2(features)
# 目标顺序: seq_len(原1), batch(原0), features(原2)
# 所以 permute 的参数是 (1, 0, 2)
model_input = batch_data.permute(1, 0, 2)
print("模型输入形状:", model_input.shape) # 输出: torch.Size([20, 10, 50])

```

## transpose()

`transpose(dim1, dim2)`：**只能交换两个维度**。它等价于 `permute()` 的一个特例。`x.transpose(1, 2)` 就等于 `x.permute(0, 2, 1)`（假设x是3维的）。

