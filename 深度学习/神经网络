## 感知机

![image-20250627155838843](assets/image-20250627155838843.png)

1、怎么输入

​	需要加上权值：w1x1 + w2x2 + w3x3

​	类比物理上的统一量纲，不同类别的值直接加在一起无意义

![image-20250627160009801](assets/image-20250627160009801.png)

2、怎么干活

![image-20250627160313206](assets/image-20250627160313206.png)

即：把内容输入到函数里面。这个函数是什么？激活函数

激活函数：从输入到输出要满足我们的需求

例如，有一个数，我们要得到他的平方，即`f(x) = x^2`

那么`f(x) = x^2`就是“有一个数，我们要得到他的平方”的激活函数



从感知机一直往下延伸，就是深度学习的内容

## 神经网络

如果只有一个机器肯定是不好用，就像大脑中只有一个神经元肯定不行。

![image-20250627161402363](assets/image-20250627161402363.png)

每个值都输入到圈中，每个输入都有权重，每个圈都有激活函数，每个都产生一个输出

那么输出又可以输入到下一层

![image-20250627161545329](assets/image-20250627161545329.png)

输出最终的y 

![image-20250627161612073](assets/image-20250627161612073.png)

这还是相乘相加的计算，“神经网络”是怎么和“人脑”的逻辑有关联的？——反向传播

## 反向传播

还回到这个东西：有一个数，我们要得到他的平方

如果数是2，你得出平方为4

如果数是15，你得出平方为225

如果数是16，你得出平方为256

但你总有记忆错乱或者笔误的时候，你把225写成了255，把256写成了266等

老师给你打了错号，**因为错误，你去比较**



比如我们现在给定了x1和x2，我们需要得到一个数为100 我们去设定权重w

一开始，我们随便设置权值，得到y 这个y肯定不是100

求误差：δ = 100-y

改正：要改正的是w

怎么修改——反向传播



 ![image-20250627163157840](assets/image-20250627163157840.png)

反向过程，我们要的输出和实际的输出之间的差值为δ，用δ乘以权重，相当于反向过程

δ1 = w1*δ

δ2 = w2*δ

**反向传播是不经过f的**

![image-20250627163525453](assets/image-20250627163525453.png)

δ5 = δ1 * w5 + δ2 * w6 + ……

到这一步就停了，前面就是输入了

求了这么多误差δ，我们的目标是修改权重w

修改权重的公式：

![image-20250627164144062](assets/image-20250627164144062.png)

输入x1 以及 那个点 都表示 w1x1 + w2x2 + w3x3 ……



修改后的w11' 就是我们想要的权重吗？很难，几率很小

w11到w11'可能修改的幅度比较大 但是w11'''' 到w11'''''修改的幅度可能比较小

也就是我们需要的是**后面那部分**不断向0靠拢

![image-20250627165045403](assets/image-20250627165045403.png)

η是常数 误差是一个影响因素 导数是一个影响因素 输入是定死的

到底由二者中的哪一个来影响？是**导数**

如果是由误差影响的，要想后面这部分趋近于0，那么误差就要趋近于0，误差趋近于0就没必要反向传播了



导数逐渐变为0的过程是梯度下降的过程

![image-20250627165444550](assets/image-20250627165444550.png)





损失函数：类似于100-y的公式

一个epoch：一个修改权值的过程，一轮



