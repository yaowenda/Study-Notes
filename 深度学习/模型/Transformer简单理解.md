## 自注意力机制

把每个句子拆分成最小的语义单位，也就是token

<img src="../assets/image-20251114094326091.png" alt="image-20251114094326091" style="zoom:50%;" />

每一个token会被编码成一个512维(256 128)的向量（[1.2, 0.3, -0.6, ... , 1.8] 512维），因为这些词向量是同时输入给模型的，所以模型本身无法判断它们的先后顺序，因此还需要给每个词向量加上一个位置编码，以此告诉模型这些词在整个句子中的顺序

<img src="../assets/image-20251114095712331.png" alt="image-20251114095712331" style="zoom: 50%;" />

这样的话我们就可以得到一个10x512的词向量组，用变量X表示——词向量组X(10*512)

<img src="../assets/image-20251114095904217.png" alt="image-20251114095904217" style="zoom:50%;" />

模型又应该怎么通过X找出每一个词之间的联系呢？答案就是让他们相互去进行计算，为了实现这一点，模型首先会用三个权重矩阵分别和每个词向量进行相乘，那么每个token就能够得到三个向量Q K V，

![image-20251114103457380](../assets/image-20251114103457380.png)

Q代表查询向量，代表当前这个词它想关注什么；K表示键向量，表示它能为其他词提供什么信息（类似于标签或索引）；V是值向量，代表这个词实际包含的信息内容，它是真正被检索和聚合的信息本身 

接下来模型将每个词的Q向量分别和这个句子中每个词的K向量进行相乘（点积），为了使结果显得更合理，会把结果除以一个常数D_k，它代表向量的维度，用来防止梯度爆炸。这样的话我们就能得到一组注意力分数：

<img src="../assets/image-20251114104016112.png" alt="image-20251114104016112" style="zoom:50%;" />

再套上一层softmax函数，就能得到一组注意力权重：

<img src="../assets/image-20251114104108562.png" alt="image-20251114104108562" style="zoom:50%;" />

权重代表当前词和其他每一个词的关联程度，例如第一个“用”字和第一个“毒”字的权重更高，而和第二个“毒”字的权重较低

接下来会把每个词的V向量和它对应的注意力权重进行相乘，再进行加权求和，得到一组新向量

<img src="../assets/image-20251114104450657.png" alt="image-20251114104450657" style="zoom:50%;" />

<img src="../assets/image-20251114104525857.png" alt="image-20251114104525857" style="zoom:50%;" />

这个新向量就能去包含句子当中的每一个词对其他词含义的理解

整个完整的计算过程称之为自注意力机制——自己和自己产生注意力，什么含义？

## 多头自注意力机制

举个例子：

<img src="../assets/image-20251114105030991.png" alt="image-20251114105030991" style="zoom:50%;" /><img src="../assets/image-20251114105043086.png" alt="image-20251114105043086" style="zoom:50%;" />

你带着问题Q进入图书馆，然后去和图书馆里的所有图书标签K进行匹配，匹配度最高的书说明它的内容最可能解决我们的问题，那对它的关注度就是最高的，于是开始花更多时间阅读它的内容V，而对于关注度较低的书，则花较少的时间进行阅读。当阅读完走出图书馆的时候，就完成了对知识的整合，你带走的新的知识就已经包含了你对这些书的理解

同理，自注意力计算之后每个token都会产生对其他token的抽象理解，这种理解其实就藏在了经过了加权的向量当中（暂时称为向量Z）。然而这种理解仍然不够深入，因为自注意力机制往往只能让一个词对其他词产生单一的关注和理解，但在现实中一个句子里的词语往往会具有多方面的语义信息，例如：

![image-20251114105755744](../assets/image-20251114105755744.png)

因此只进行一次自注意力计算会显得有些局限，于是Transformer干脆将最初的输入向量X拆分成8个头，那么原本512维的X向量就被拆分成8个64维向量

<img src="../assets/image-20251114110025801.png" alt="image-20251114110025801" style="zoom:50%;" />

自注意计算也由这8个头独立进行，最后再将这8个计算的结果拼接起来

<img src="../assets/image-20251114110218639.png" alt="image-20251114110218639" style="zoom:50%;" />

<img src="../assets/image-20251114110345267.png" alt="image-20251114110345267" style="zoom:33%;" /><img src="../assets/image-20251114110411040.png" alt="image-20251114110411040" style="zoom:33%;" />

这是向量Z就包含了8个头独立计算的结果，能从多个角度整合复杂的语义信息

这是多头自注意力机制

但这也不够，一个多头自注意力机制理解一个句子还可以，理解一段话或者一篇文章就不行了，因此一般多层：

![image-20251114110659534](../assets/image-20251114110659534.png)

## 编码器

多层神经网络必然会带来一个问题：梯度消失

在深层网络里，某一层的最优变换若是“恒等映射”（即希望输出≈输入），传统结构必须让 F(x) 强行学到 x 本身。当 F(x) 近似 0 时，前向信息被阻断，反向梯度也因链式连乘而指数衰减，出现梯度消失，看上去只有靠后的层在更新。残差网络把这一层的输出改成 F(x)+x：前向多了一条“捷径”，即使 F(x)=0 也能无损传递 x；反向时梯度至少保留“+1”项，使前端层总能收到更新信号。于是网络只需把 F(x) 学成 0 即可完成恒等映射，既避免了信息丢失，又提供了不经过权重矩阵的梯度高速通道，这就是 ResNet 的核心思想。

因此我们在多头自注意力层后面加上残差网络，用来保证深层网络的训练稳定性，然后我们还需要对每一层输出的值进行归一化，让向量中的数值保持在一个合理的范围内

![image-20251114150857608](../assets/image-20251114150857608.png)

归一化后的输出向量，我们称之为A，

接下来还需要在每一层加入一个前馈神经网络，他会对A向量中的每一个元素使用激活函数去进行非线性变换（因为在之前的计算当中，无论多复杂，函数的输出结果本质上还是线性的）

![image-20251114151142457](../assets/image-20251114151142457.png)

在经过FFN之后，出于同样的目的（稳定训练），再加一个残差网络和归一化层，经过N层这样的结果之后，最终输出一个向量X_out

 ![image-20251114151428104](../assets/image-20251114151428104.png)

编码器的主要功能是 负责去理解我们输入的语言。现在我们已经理解了输入的句子了，接下来就要根据我们的理解来生成对应的翻译，负责生成的模块就叫做解码器

## 解码器

解码器会接受两个输入，第一个是用于“瞻前”的内容，也就是编码器的输出X_out，第二个是用于“顾后”的，已生成的单词序列。

<img src="../assets/image-20251114152943172.png" alt="image-20251114152943172" style="zoom:50%;" />

模型是逐个生成单词的，每生成一个新的单词，都要把之前已经生成的整个序列重新输入解码器，以此来预测下一个token。如果模型还没有生成任何的token，那就先把一个起始符`[<sos>]`输入进去。然后输入的向量首先会进入掩码多头自注意力机制层，

<img src="../assets/image-20251114153059332.png" alt="image-20251114153059332" style="zoom:50%;" />

这个掩码多头自注意力机制的计算的之前的多头自注意力基本是一致的，都是对输入的每一个token计算他们之间的注意力权重

**掩码：**

___

问得非常好！这个问题直击了注意力机制设计的核心。
沿着最后一个维度做softmax，不是一个随意的决定，而是由注意力机制的根本目的决定的。这个目的就是：为每一个查询（Query）计算出它应该如何“分配注意力”到所有的键（Key）上。
我们还是用之前“写论文查文献”的比喻来解释，这会非常直观。
1. 关注的主体是“Query”
在注意力机制中，一切计算都围绕着 Query 展开。可以把它理解成“我当前的需求”或“我当前的问题”。
对于每一个 Query，我们都想得到一个属于它自己的注意力权重分布。
这个权重分布告诉我们，为了回答“这个Query”，我们应该把多少注意力放在第1个Key上，多少放在第2个Key上，...，多少放在最后一个Key上。
2. scores 矩阵的结构
我们先回顾一下 scores 矩阵的形状：(batch_size, seq_len_q, seq_len_k)。
这个三维矩阵可以看作是一个批次里，包含了 batch_size 个二维矩阵。我们只看其中一个二维矩阵，它的形状是 (seq_len_q, seq_len_k)。
这个二维矩阵的每一行，都代表一个Query。
每一行里的 seq_len_k 个数值，分别代表这个Query与所有 seq_len_k 个Key的相似度得分。
      Key_1  Key_2  Key_3  ...  Key_k
Query_1 [ 0.2,  1.5, -0.8, ...,  0.9 ]  <-- Query_1 的得分
Query_2 [ 0.7, -0.3,  2.1, ...,  0.1 ]  <-- Query_2 的得分
...
Query_q [ 1.1,  0.4, -0.2, ...,  1.8 ]  <-- Query_q 的得分
3. 为什么是 dim=-1？
现在，关键问题来了：我们应该对哪个维度做softmax，才能得到“每个Query自己的权重分布”？
如果我们对行（dim=-2）做softmax：那意味着我们把 Query_1、Query_2... Query_q 的得分放在一起比较。这就像在问：“在所有Query中，哪个Query和Key_1最相关？” 这显然不是我们想要的。我们关心的是单个Query内部的权重分配。
如果我们对列（dim=-1）做softmax：这正中下怀！看上面表格的第一行，我们对 [0.2, 1.5, -0.8, ..., 0.9] 这一行做softmax，就会得到一个所有元素加起来为1的概率分布。这个分布就完美地回答了：“对于 Query_1 来说，它应该给每个Key分配多少注意力？”
对每一行都重复这个操作，我们就会得到 seq_len_q 个独立的、属于每个Query的注意力权重分布。
4. 数学上的直观理解
softmax 函数的作用是将一组任意实数转换成概率分布。它的公式是：
$$ \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}} $$
注意看分母的 求和符号 $\sum_j$。当我们对 dim=-1（即每一行）进行softmax时，这个求和操作是在该行的所有元素上进行的。这保证了对于同一个Query，所有Key的权重之和为1。
总结
一句话概括：
因为我们的目标是为每一个Query，在所有的Key上计算一个概率分布。scores矩阵的最后一维（dim=-1）正好代表了“所有的Key”，所以我们必须沿着这个维度做softmax，才能为每一个Query生成一个有效的注意力权重向量。
这个设计非常巧妙，它将“计算注意力”这个抽象概念，完美地编码到了张量的维度操作中。
