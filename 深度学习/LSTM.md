## RNN问题：

- 无法辨别不同时间点上的信息对未来的不同影响
- 无法辨别不同时间步上的标签最重要的信息是哪些
- 虽然理论上最后一个时间步中包含了所有时间点上的信息，但是真正有影响力的只有非常接近最后一个时间步的几个时间步而已，大部分信息被RNN以往，导致难处理长序列

## LSTM：为循环神经网络赋予选择性记忆和选择性传递信息的能力

- 循环网络必须自行选择吸纳多少新信息，
- 循环网络必须自行选择遗忘多少历史信息，
- 循环网络必须自行判断、对当前时间步的预测来说最重要的信息是哪些，并将该信息输出给当前时间步，这样既可以保证当前时间步上的预测是最高效的，也不会影响向下一个时间步传递的信息。

![image-20251203210146790](assets/image-20251203210146790.png)

记忆细胞是LSTM特有的结构

记忆细胞是LSTM的基本计算单元，在记忆细胞中，我们分割长期信息与短期信息，同时赋予循环网络对信息做选择的能力。在之前我们提到，循环网络必须自行决定哪些长期信息会被传递下去，哪些短期信息对当前的预测最为有效，因此在记忆细胞当中，LSTM设置了两个关键变量：

- 隐藏状态h：主要负责记忆短期信息、尤其是当前时间步信息
- 细胞状态C：主要负责长期记忆

横向上可以分割为C的传递和h的传递，纵向上可以分为三个路径：

1. 帮助循环网络**选择吸纳多少新信息的输入门**
2. 帮助循环网络**选择遗忘多少历史信息的遗忘门**
3. 帮助循环网络**选择出对当前时间步的预测来说最重要的信息、并将该信息输出给当前时间步的输出门**

![image-20251203210832827](assets/image-20251203210832827.png)

循环神经网络结构：

<img src="assets/image-20251203211427595.png" alt="image-20251203211427595" style="zoom:50%;" />

模型预测的标签是在每一个时间步进行输出还是在最后一个时间步的输出层输出？

和任务有关，假如是情感分类任务，对一句话的每一个单词做情感分类的话（对每一个样本进行分析），那么就是在每一个时间步的输出层进行输出；如果是生成式任务，需要看完输入才给输出，那么就是在最后一个时间步的输出层输出

如果是对每一个样本进行分析，那么每一个时间步的输出都应该尽量准确，梯度的迭代应该向这个方向迭代，但同时模型还是要向下一步传播的，还要兼顾对后面时间步最佳信息的传输，这两者可能是矛盾的，因为对当前时间步最好的信息不一定是最适合传递下去的信息，RNN无法处理这一点，因此LSTM才改进至**“帮助循环网络选择出对当前时间步的预测来说最重要的信息、并将该信息输出给当前时间步的输出门”**，至于哪些信息传递给未来最好，在LSTM当中是单独计算的。所以才要分隔长期和短期

输入门和遗忘门是为了构建长期传递的信息，而输出门主要是为了输出当前时间步最重要的信息

## 三个门如何工作

### 遗忘门

遗忘门是决定要留下多少长期信息C的关键计算单元，其数学本质是令上一个时间步传入的`C_{t-1}`乘以一个[0,1]之间的比例，以此筛选掉部分旧信息。在这个计算过程中，假设遗忘门令`C_{t-1}`乘以0.7，那就是说遗忘门决定了要保留70%的历史信息，遗忘30%的历史信息，这30%的信息空间就可以留给全新的信息来使用。

![image-20251203214041094](D:/typora/%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/assets/image-20251203214041094.png)

那这个比例是如何被计算出来的呢？如图所示，遗忘门会参考当前时间步的信息`X_t`,与上一个时间步的短时信息`h_t-1` 来计算该比例，其中 σ是sigmoid函数，`W_f`是动态影响最终权重大小的参数，`f_t`就是[0,1]之间的、用于影响`C_t-1`的比例。

在LSTM的设计逻辑之中，考虑X和`h_t-1`实际是在考虑离当前时间步最近的上下文信息，而权重`w_f`会受到损失函数和算法整体表现的影响，不断调节遗忘门中计算出的比例f的大小，因此遗忘门能够结合上下文信息、损失函数传来的梯度信息、以及历史信息共同计算出全新的、被留下的长期记忆`C_t`。这个流程在实践中被证明是十分有效的。

### 输入门